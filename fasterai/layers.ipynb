{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "layers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaiAnant/MangaChroma/blob/master/fasterai/layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbmtzdByEkcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.layers import *\n",
        "from fastai.torch_core import *\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3KLWg5oKLJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_conv_layer(ni:int, nf:int, ks:int=3, stride:int=1, padding:int=None, bias:bool=None, is_1d:bool=False,\n",
        "               norm_type:Optional[NormType]=NormType.Batch,  use_activ:bool=True, leaky:float=None,\n",
        "               transpose:bool=False, init:Callable=nn.init.kaiming_normal_, self_attention:bool=False,\n",
        "               extra_bn:bool=False):\n",
        "    \"Create a sequence of convolutional (`ni` to `nf`), ReLU (if `use_activ`) and batchnorm (if `bn`) layers.\"\n",
        "    if padding is None: padding = (ks-1)//2 if not transpose else 0\n",
        "    bn = norm_type in (NormType.Batch, NormType.BatchZero) or extra_bn==True\n",
        "    if bias is None: bias = not bn #dont add bia if bn is implemented\n",
        "    conv_func = nn.ConvTranspose2d if transpose else nn.Conv1d if is_1d else nn.Conv2d\n",
        "    #init_default initialises the weigts of the model massed to it according to the function \"init\"\n",
        "    conv = init_default(conv_func(ni, nf, kernel_size=ks, bias=bias, stride=stride, padding=padding), init)\n",
        "    if   norm_type==NormType.Weight:   conv = weight_norm(conv) #??\n",
        "    elif norm_type==NormType.Spectral: conv = spectral_norm(conv) #??\n",
        "    layers = [conv]\n",
        "    if use_activ: layers.append(relu(True, leaky=leaky))\n",
        "    if bn: layers.append((nn.BatchNorm1d if is_1d else nn.BatchNorm2d)(nf))\n",
        "    \n",
        "    if self_attention: layers.append(SelfAttention(nf)) #??\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}